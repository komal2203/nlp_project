{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9bc60c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Prepare data â€“ note that some models do not have values for certain metrics\n",
    "models = ['Random Forest', 'Neural Network', 'Gradient Boosting', 'NN + MHA', 'BERT Model', 'Logistic Regression']\n",
    "\n",
    "# Time Metrics\n",
    "training_time = [1.06, 1032.62, 12.4493, 2749.52, 32.70, 29.26]\n",
    "inference_time = [0.01, 0.68, 0.0016, 1.30, 0.23, 0.00]\n",
    "\n",
    "# Accuracy Metrics (in percentages)\n",
    "training_accuracy = [99.62, 98.31, 95.68, 98.22, 95.72, 100.00]\n",
    "validation_accuracy = [88.45, 90.42, 91.83, 90.42, 90.14, 89.01]\n",
    "# For the BERT model, test accuracy equals training accuracy for this comparison\n",
    "\n",
    "# Loss Metrics (where available; use NaN for missing values)\n",
    "training_loss = [0.0750, 0.1588, 0.1475, 0.6900, 0.4777, 0.0046]\n",
    "# Random Forest and Logistic Regression don't have validation loss values provided\n",
    "validation_loss = [np.nan, 0.4358, 0.2398, 1.2424, 0.6893, np.nan]\n",
    "\n",
    "# Model Complexity (using trainable parameters or equivalent info)\n",
    "# For Random Forest and Gradient Boosting, we note ensemble details instead.\n",
    "complexity_info = {\n",
    "    'Random Forest': '19 trees; Avg depth: 121.74',\n",
    "    'Neural Network': '13,558,818',\n",
    "    'Gradient Boosting': '100 estimators',\n",
    "    'NN + MHA': '13,564,154',\n",
    "    'BERT Model': '240,162',\n",
    "    'Logistic Regression': '52,795'\n",
    "}\n",
    "\n",
    "# Create DataFrames for plotting\n",
    "df_time = pd.DataFrame({'Model': models,\n",
    "                        'Training Time (s)': training_time,\n",
    "                        'Inference Time (s)': inference_time})\n",
    "df_accuracy = pd.DataFrame({'Model': models,\n",
    "                            'Training Accuracy (%)': training_accuracy,\n",
    "                            'Validation Accuracy (%)': validation_accuracy})\n",
    "df_loss = pd.DataFrame({'Model': models,\n",
    "                        'Training Loss': training_loss,\n",
    "                        'Validation Loss': validation_loss})\n",
    "\n",
    "# Set seaborn style\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "### Plot 1: Training Time and Inference Time ###\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "sns.barplot(x='Model', y='Training Time (s)', data=df_time, ax=axes[0])\n",
    "axes[0].set_title(\"Training Time by Model\")\n",
    "axes[0].set_xticklabels(axes[0].get_xticklabels(), rotation=45, ha='right')\n",
    "\n",
    "sns.barplot(x='Model', y='Inference Time (s)', data=df_time, ax=axes[1])\n",
    "axes[1].set_title(\"Inference Time by Model\")\n",
    "axes[1].set_xticklabels(axes[1].get_xticklabels(), rotation=45, ha='right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "### Plot 2: Training vs. Validation Accuracy ###\n",
    "df_acc_melted = df_accuracy.melt(id_vars='Model', var_name='Metric', value_name='Accuracy')\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(x='Model', y='Accuracy', hue='Metric', data=df_acc_melted)\n",
    "plt.title(\"Training vs. Validation Accuracy\")\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.ylim(80, 105)\n",
    "plt.legend(title='Metric')\n",
    "plt.show()\n",
    "\n",
    "### Plot 3: Training vs. Validation Loss ###\n",
    "# Exclude models with NaN validation loss to make the graph cleaner\n",
    "df_loss_clean = df_loss.dropna()\n",
    "df_loss_melted = df_loss_clean.melt(id_vars='Model', var_name='Loss Metric', value_name='Loss')\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(x='Model', y='Loss', hue='Loss Metric', data=df_loss_melted)\n",
    "plt.title(\"Training vs. Validation Loss\")\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.legend(title='Loss Metric')\n",
    "plt.show()\n",
    "\n",
    "### Plot 4: Model Complexity Comparison ###\n",
    "# Since complexity for ensembles (Random Forest, Gradient Boosting) is descriptive,\n",
    "# we create a table using matplotlib's table functionality.\n",
    "fig, ax = plt.subplots(figsize=(8, 2))\n",
    "ax.axis('tight')\n",
    "ax.axis('off')\n",
    "table_data = [[model, complexity_info[model]] for model in models]\n",
    "table = ax.table(cellText=table_data, colLabels=[\"Model\", \"Complexity\"], cellLoc='center', loc='center')\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(10)\n",
    "table.scale(1, 2)\n",
    "plt.title(\"Model Complexity Comparison\", fontweight=\"bold\", pad=20)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd8f658",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "covsent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
